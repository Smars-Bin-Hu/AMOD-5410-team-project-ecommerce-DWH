# AMOD-5410-team-project-ecommerce-DWH

## ğŸ“– Introduction

**Name:**

**Background:**

**Goal:** To build a data batch processing datawarehouse,including data ingestion, cleaning, storage, modelling, analytis and scheduler. Use tech stach as below.

## ğŸš€ Tech Stack

- **Data Source(OLTP):** Oracle Database
- **Data Extraction, Load:** Airflow + JDBC or Apache Sqoop
- **Data Storage and Resources Management:** Apache Hadoop(HDFS, Yarn)
- **Data Warehousing:** Apache Hive, MySQL(Metastore), Dimension Modelling
- **Data Transform:** Apache Spark(PySpark), SparkSQL
- **Scheduler:** Airflow
- **OLAP engine:** MySQL or clickhouse
- **Data Application Layer**: PowerBI or tableau

## ğŸ“ Project Directory

```bash
/bigdata-datawarehouse-project
â”‚â”€â”€ /docs                    # docs (technologies architecture diagram, desgin, README)
â”‚â”€â”€ /data_pipeline           # data pipeline code (ETL/ELT Logic, output)
â”‚â”€â”€ /warehouse_modeling      # DWH modellingï¼ˆHive/SparkSQL etc.ï¼‰
â”‚â”€â”€ /batch_processing        # Data Batch processing (Hadoop, Hive, Spark)
â”‚â”€â”€ /scheduler               # Task Scheduler(Airflow/DolphinScheduler)
â”‚â”€â”€ /infra                   # infrastructure deployment(Docker, Kubernetes)
â”‚â”€â”€ /notebooks               # Jupyter Notebook
â”‚â”€â”€ /tests                   # Testing code
â”‚â”€â”€ /scripts                 # deployment and operations script and command
â”‚â”€â”€ README.md                # Introduction about project
â”‚â”€â”€ docker-compose.yml       # Docker Compose to launch the project
â”‚â”€â”€ requirements.txt         # Python dependencies
```

## ğŸ’ª Quick Start


Add hosts to your local `/etc/hosts`

```
# for docker network
127.0.0.1   hadoop-master 
127.0.0.1   hadoop-worker1
127.0.0.1   hadoop-worker2
127.0.0.1   mysql-hive-metastore
127.0.0.1   hive
127.0.0.1   spark
127.0.0.1   oracle-oltp
```
