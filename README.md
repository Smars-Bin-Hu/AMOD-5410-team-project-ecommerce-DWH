# AMOD-5410-team-project-ecommerce-DWH

## ðŸ“– Introduction

**Name:**

**Background:**

**Goal:** To build a data batch processing datawarehouse, including data ingestion, cleaning, storage, modelling, analytis and scheduler. Use tech stach as below.

## ðŸš€ Tech Stack

- **Data Source(OLTP):** Oracle
- **Data Extraction, Load:** Maxwell
- **Data Storage and Resources Management:** HDFS, Yarn
- **Data Warehousing:** Apache Hive, MySQL(Metastore), Dimension Modelling
- **Data Transform:** PySpark, SparkSQL
- **Scheduler:** Airflow
- **OLAP engine:** clickhouse
- **Data Application Layer**: PowerBI

## ðŸ“ Project Directory

```bash
/bigdata-datawarehouse-project
â”‚â”€â”€ /docs                    # docs (all business and technologies documents about this project)
â”‚â”€â”€ /data_pipeline           # data pipeline code (ETL/ELT Logic, output)
â”‚â”€â”€ /warehouse_modeling      # DWH modellingï¼ˆHive/SparkSQL etc.ï¼‰
â”‚â”€â”€ /batch_processing        # Data Batch processing (Hadoop, Hive, Spark)
â”‚â”€â”€ /scheduler               # Task Scheduler(Airflow/DolphinScheduler)
â”‚â”€â”€ /infra                   # infrastructure deployment(Docker, Kubernetes)
â”‚â”€â”€ /notebooks               # Jupyter Notebook
â”‚â”€â”€ /tests                   # Testing code
â”‚â”€â”€ /scripts                 # deployment and operations script and command
â”‚â”€â”€ README.md                # Introduction about project
â”‚â”€â”€ docker-compose.yml       # Docker Compose to launch the project
â”‚â”€â”€ requirements.txt         # Python dependencies
```

## ðŸ’ª Quick Start

Add hosts to your local `/etc/hosts`

```
# for docker network
127.0.0.1   hadoop-master 
127.0.0.1   hadoop-worker1
127.0.0.1   hadoop-worker2
127.0.0.1   mysql-hive-metastore
127.0.0.1   hive
127.0.0.1   spark
127.0.0.1   oracle-oltp
127.0.0.1   airflow
```

## ðŸ“Œ Project Documents

> [!NOTE]
> Click this link to the document navigation map in the /docs: [Document Navigation](./docs/README.md)
