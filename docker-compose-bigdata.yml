networks:
  bigdata-net:
    name: bigdata-net
    driver: bridge
    ipam:
      config:
        - subnet: ${HADOOP_SUBNET}  # subnet
          gateway: ${HADOOP_GATEWAY}    # GATEWAY

services:
  # ========== 1) Hadoop Master ==========
  # -------------------------------------------------
  # Hadoop Master: start ZK、JournalNode、NameNode、ZKFC、ResourceManager、DN/NM
  # -------------------------------------------------
  hadoop-master:
    container_name: hadoop-master               
    image:   ${HADOOP_MASTER_IMAGE}
    hostname: hadoop-master
    networks:
      bigdata-net:
        ipv4_address: ${HADOOP_MASTER_IP}
    environment: # Environment Variables
      - HADOOP_MASTER_HOST=hadoop-master
    ports:
      - "${LOCALHOST_SSH_PORT_HADOOP_MASTER}:${CONTAINER_INTERNAL_SSH_PORT}"    # SSH
      - "${LOCALHOST_HDFS_NN1_WEB_UI}:${CONTAINER_HDFS_WEB_UI}"
      - "${LOCALHOST_YARN_RM1_WEB_UI}:${CONTAINER_YARN_RM1_WEB_UI}"
      - "${LOCALHOST_YARN_NM1_LOG_WEB_UI}:${CONTAINER_YARN_LOG_WEB_UI}"
      - "${LOCALHOST_HDFS_PORT}:${CONTAINER_INTERNAL_HDFS_PORT}" # HDFS for hive
      - "${LOCALHOST_YARN_JOB_HISTORY_SERVER}:${CONTAINER_YARN_JOB_HISTORY_SERVER}"
    extra_hosts:
      - "hadoop-worker2:${HADOOP_WORKER2_IP}"
      - "hadoop-worker1:${HADOOP_WORKER1_IP}"
    stdin_open: true     # keep the container runing
    tty: true     # keep the container runing
    command: >
        bash -c "
          service ssh start;
          tail -f /dev/null
        "
    volumes:
      # /etc/hosts
      - ./src/infra/hadoop-config/hosts/hosts:${HADOOP_CLUSTER_HOSTS_FILE}
      # ssh configs
      - ./src/infra/ssh-config/config:${HADOOP_CLUSTER_SSH_CONFIG_FILE}
      # hadoop scripts
      - ./src/scripts/hadoop-master/my-start-zk.sh:${HADOOP_CLUSTER_BIN_PATH}/my-start-zk.sh
      - ./src/scripts/hadoop-master/my-start-NN-ZKFC.sh:${HADOOP_CLUSTER_BIN_PATH}/my-start-NN-ZKFC.sh
      - ./src/scripts/hadoop-master/my-start-RM.sh:${HADOOP_CLUSTER_BIN_PATH}/my-start-RM.sh
      - ./src/scripts/hadoop-master/my-start-NM-DN.sh:${HADOOP_CLUSTER_BIN_PATH}/my-start-NM-DN.sh
      - ./src/scripts/hadoop-master/my-start-historyserver.sh:${HADOOP_CLUSTER_BIN_PATH}/my-start-historyserver.sh
      - ./src/scripts/hadoop-master/my-stop-hadoop-cluster.sh:${HADOOP_CLUSTER_BIN_PATH}/my-stop-hadoop-cluster.sh
      # hadoop cluster configs
      - ./src/infra/hadoop-config/core-site.xml:${HADOOP_HOME}/etc/hadoop/core-site.xml
      - ./src/infra/hadoop-config/hadoop-env.sh:${HADOOP_HOME}/etc/hadoop/hadoop-env.sh
      - ./src/infra/hadoop-config/hdfs-site.xml:${HADOOP_HOME}/etc/hadoop/hdfs-site.xml
      - ./src/infra/hadoop-config/yarn-site.xml:${HADOOP_HOME}/etc/hadoop/yarn-site.xml
      - ./src/infra/hadoop-config/mapred-site.xml:${HADOOP_HOME}/etc/hadoop/mapred-site.xml
      - ./src/infra/hadoop-config/yarn-env.sh:${HADOOP_HOME}/etc/hadoop/yarn-env.sh
      - ./src/infra/hadoop-config/workers:${HADOOP_HOME}/etc/hadoop/workers
      - ./src/infra/zookeeper-config/zoo.cfg:${ZOOKEEPER_HOME}/conf/zoo.cfg
      - ./src/infra/zookeeper-config/dataDir.1/myid:${ZOOKEEPER_HOME}/data/myid
      # hdfs metadata
#      - ./src/infra/hdfs-data/master:${HADOOP_HOME}/data
      # local mount - for big volume storage
      - ${HADOOP_DATA_LOCAL_MOUNT_PATH}:/data
      
  # ========== 2) Hadoop Worker1 ==========
  # -------------------------------------------------
  # Hadoop Worker1: start ZK、JournalNode、ResourceManager、DN/NM
  # -------------------------------------------------
  hadoop-worker1:
    container_name: hadoop-worker1
    image: ${HADOOP_WORKER1_IMAGE}
    hostname: hadoop-worker1
    networks:
      bigdata-net:
        ipv4_address: ${HADOOP_WORKER1_IP}
    environment:
      - HADOOP_MASTER_HOST=hadoop-master
    ports:
      - "${LOCALHOST_SSH_PORT_HADOOP_WORKER1}:${CONTAINER_INTERNAL_SSH_PORT}"    # SSH
      - "${LOCALHOST_YARN_RM2_WEB_UI}:${CONTAINER_YARN_RM2_WEB_UI}"
      - "${LOCALHOST_YARN_NM2_LOG_WEB_UI}:${CONTAINER_YARN_LOG_WEB_UI}"
    extra_hosts:
      - "hadoop-worker2:${HADOOP_WORKER2_IP}"
      - "hadoop-master:${HADOOP_MASTER_IP}"
    stdin_open: true     # keep the container runing
    tty: true     # keep the container runing
    command: >
      bash -c "service ssh start;
              tail -f /dev/null" 
    volumes:
      # /etc/hosts
      - ./src/infra/hadoop-config/hosts/hosts:${HADOOP_CLUSTER_HOSTS_FILE}
      # ssh configs
      - ./src/infra/ssh-config/config:${HADOOP_CLUSTER_SSH_CONFIG_FILE}
      # hadoop scripts
      - ./src/scripts/hadoop-worker1/my-start-zk.sh:${HADOOP_CLUSTER_BIN_PATH}/my-start-zk.sh
      - ./src/scripts/hadoop-worker1/my-start-RM.sh:${HADOOP_CLUSTER_BIN_PATH}/my-start-RM.sh
      - ./src/scripts/hadoop-worker1/my-start-NM-DN.sh:${HADOOP_CLUSTER_BIN_PATH}/my-start-NM-DN.sh
      - ./src/scripts/hadoop-worker1/my-stop-hadoop-cluster.sh:${HADOOP_CLUSTER_BIN_PATH}/my-stop-hadoop-cluster.sh
      # hadoop cluster configs
      - ./src/infra/hadoop-config/core-site.xml:${HADOOP_HOME}/etc/hadoop/core-site.xml
      - ./src/infra/hadoop-config/hadoop-env.sh:${HADOOP_HOME}/etc/hadoop/hadoop-env.sh
      - ./src/infra/hadoop-config/hdfs-site.xml:${HADOOP_HOME}/etc/hadoop/hdfs-site.xml
      - ./src/infra/hadoop-config/mapred-site.xml:${HADOOP_HOME}/etc/hadoop/mapred-site.xml
      - ./src/infra/hadoop-config/yarn-site.xml:${HADOOP_HOME}/etc/hadoop/yarn-site.xml
      - ./src/infra/hadoop-config/workers:${HADOOP_HOME}/etc/hadoop/workers
      - ./src/infra/hadoop-config/yarn-env.sh:${HADOOP_HOME}/etc/hadoop/yarn-env.sh
      - ./src/infra/zookeeper-config/zoo.cfg:${ZOOKEEPER_HOME}/conf/zoo.cfg
      - ./src/infra/zookeeper-config/dataDir.2/myid:${ZOOKEEPER_HOME}/data/myid
      # hdfs metadata
#      - ./src/infra/hdfs-data/worker1:${HADOOP_HOME}/data
      # local mount - for big volume storage
      - ${HADOOP_DATA_LOCAL_MOUNT_PATH}:/data

  # ========== 3) Hadoop Worker2 ==========
  # -------------------------------------------------
  # Hadoop Master: start ZK、JournalNode、NameNode、ZKFC、DN/NM
  # -------------------------------------------------
  hadoop-worker2:
    container_name: hadoop-worker2
    image: ${HADOOP_WORKER2_IMAGE}
    hostname: hadoop-worker2
    networks:
      bigdata-net:
        ipv4_address: ${HADOOP_WORKER2_IP}
    environment:
      - HADOOP_MASTER_HOST=hadoop-master
    ports:
      - "${LOCALHOST_SSH_PORT_HADOOP_WORKER2}:${CONTAINER_INTERNAL_SSH_PORT}"    # SSH
      - "${LOCALHOST_HDFS_NN2_WEB_UI}:${CONTAINER_HDFS_WEB_UI}"
      - "${LOCALHOST_YARN_NM3_LOG_WEB_UI}:${CONTAINER_YARN_LOG_WEB_UI}"
    extra_hosts:
      - "hadoop-master:${HADOOP_MASTER_IP}"
      - "hadoop-worker1:${HADOOP_WORKER1_IP}"
    stdin_open: true     # keep the container runing
    tty: true     # keep the container runing
    command: >
      bash -c "service ssh start;
              tail -f /dev/null" 
    volumes:
      # /etc/hosts
      - ./src/infra/hadoop-config/hosts/hosts:${HADOOP_CLUSTER_HOSTS_FILE}
      # ssh configs
      - ./src/infra/ssh-config/config:${HADOOP_CLUSTER_SSH_CONFIG_FILE}
      # hadoop scripts
      - ./src/scripts/hadoop-worker2/my-start-zk.sh:${HADOOP_CLUSTER_BIN_PATH}/my-start-zk.sh
      - ./src/scripts/hadoop-worker2/my-start-NN-ZKFC.sh:${HADOOP_CLUSTER_BIN_PATH}/my-start-NN-ZKFC.sh
      - ./src/scripts/hadoop-worker2/my-start-NM-DN.sh:${HADOOP_CLUSTER_BIN_PATH}/my-start-NM-DN.sh
      - ./src/scripts/hadoop-worker2/my-stop-hadoop-cluster.sh:${HADOOP_CLUSTER_BIN_PATH}/my-stop-hadoop-cluster.sh
      # hadoop cluster configs
      - ./src/infra/hadoop-config/core-site.xml:${HADOOP_HOME}/etc/hadoop/core-site.xml
      - ./src/infra/hadoop-config/hadoop-env.sh:${HADOOP_HOME}/etc/hadoop/hadoop-env.sh
      - ./src/infra/hadoop-config/hdfs-site.xml:${HADOOP_HOME}/etc/hadoop/hdfs-site.xml
      - ./src/infra/hadoop-config/mapred-site.xml:${HADOOP_HOME}/etc/hadoop/mapred-site.xml
      - ./src/infra/hadoop-config/workers:${HADOOP_HOME}/etc/hadoop/workers
      - ./src/infra/hadoop-config/yarn-site.xml:${HADOOP_HOME}/etc/hadoop/yarn-site.xml
      - ./src/infra/hadoop-config/yarn-env.sh:${HADOOP_HOME}/etc/hadoop/yarn-env.sh
      - ./src/infra/zookeeper-config/zoo.cfg:${ZOOKEEPER_HOME}/conf/zoo.cfg
      - ./src/infra/zookeeper-config/dataDir.3/myid:${ZOOKEEPER_HOME}/data/myid
      # hdfs metadata
#      - ./src/infra/hdfs-data/worker2:${HADOOP_HOME}/data
      # local mount - for big volume storage
      - ${HADOOP_DATA_LOCAL_MOUNT_PATH}:/data

  # ========== 4) MySQL (metastore for Hive) ==========
  mysql:
    container_name: mysql-hive-metastore
    image: ${MYSQL_HIVE_METASTORE_IMAGE}
    hostname: mysql-hive-metastore
    networks:
      bigdata-net:
        ipv4_address: ${MYSQL_IP}
    environment:
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}  # for local testing only
    ports:
      - "${LOCALHOST_MYSQL}:${CONTAINER_MYSQL}"
    volumes:
      - ./src/infra/mysql-metastore-dump:/var/opt
#      - ./src/infra/mysql-data:/var/lib/mysql   # Local Persistence

  # ========== 5) Hive Service (metastore + hiveserver2) ==========
  hive:
    container_name: hive
    image: ${HIVE_IMAGE}
    platform: linux/amd64
    hostname: hive
    networks:
      bigdata-net:
        ipv4_address: ${HIVE_IP}
    depends_on:
      - mysql
      - hadoop-master
    ports:
      - "${LOCALHOST_HIVE_SERVER2_PORT}:${CONTAINER_HIVE_SERVER2_PORT}"  # HiveServer2
      - "${LOCALHOST_HIVE_SERVER2_2ND_PORT}:${CONTAINER_HIVE_SERVER2_2ND_PORT}"  # HiveServer2 2nd
      - "${LOCALHOST_HIVE_METASTORE_PORT}:${CONTAINER_HIVE_METASTORE_PORT}"    # Metastore Thrift
    volumes:
      - ./src/infra/hive-config/hive-site.xml:${HIVE_HOME}/conf/hive-site.xml
      - ./src/infra/hive-config/hiveservices.sh:${HIVE_HOME}/bin/hiveservices.sh
      - ./src/infra/hive-config/mysql-connector-java-8.0.30.jar:${HIVE_HOME}/lib/mysql-connector-java-8.0.30.jar # JDBC
      - ./src/infra/hive-config/hadoop-env.sh:${HIVE_HADOOP_HOME}/etc/hadoop/hadoop-env.sh
      - ./src/infra/hive-config/core-site.xml:${HIVE_HADOOP_HOME}/etc/hadoop/core-site.xml
      - ./src/infra/hive-config/hdfs-site.xml:${HIVE_HADOOP_HOME}/etc/hadoop/hdfs-site.xml
      - ./src/infra/hive-config/core-site.xml:${HIVE_HOME}/conf/core-site.xml
      - ./src/infra/hive-config/hdfs-site.xml:${HIVE_HOME}/conf/hdfs-site.xml
    environment:
      # connect metastore
      - HIVE_METASTORE_DB_HOST=${METASTORE_HOST}
      - HIVE_METASTORE_DB_USER=${METASTORE_USERNAME}
      - HIVE_METASTORE_DB_PASS=${METASTORE_PASSWORD}
    # command: >
    #   bash -c "/opt/hive/bin/hiveservices.sh start; 
    #           tail -f /dev/null" 

  # ========== 6) Spark on YARN ==========
  spark:
    container_name: spark
    image: ${SPARK_IMAGE}
    hostname: spark
    networks:
      bigdata-net:

        ipv4_address: ${SPARK_IP}
    depends_on: # launch the hadoop cluster first
      - hadoop-master
      - hadoop-worker1
      - hadoop-worker2
      - hive
    ports:
      - "${LOCALHOST_SSH_PORT_SPARK}:${CONTAINER_INTERNAL_SSH_PORT}"    # SSH
      - "${LOCALHOST_SPARK_WEB_UI}:${CONTAINER_SPARK_WEB_UI}"  # Spark UI
      - "${LOCALHOST_SPARK_MASTER}:${CONTAINER_SPARK_MASTER}"  # Spark Master
      - "${LOCALHOST_SPARK_THRIFT_SERVER_PORT}:${CONTAINER_SPARK_THRIFT_SERVER_PORT}" # ThriftServer
    volumes:
      - ./src/infra/spark-config/spark-defaults.conf:${SPARK_HOME}/conf/spark-defaults.conf
      - ./src/infra/spark-config/spark-env.sh:${SPARK_HOME}/conf/spark-env.sh
      - ./src/infra/spark-config/log4j2.properties:${SPARK_HOME}/conf/log4j2.properties
      - ./src/infra/hadoop-config/core-site.xml:${SPARK_HOME}/conf/core-site.xml
      - ./src/infra/hadoop-config/yarn-site.xml:${SPARK_HOME}/conf/yarn-site.xml
      - ./src/infra/hadoop-config/hdfs-site.xml:${SPARK_HOME}/conf/hdfs-site.xml
    environment:
      - SPARK_MASTER=hadoop-master
      - SPARK_WORKLOAD=yarn
    command: >
      bash -c "
              service ssh start;
              tail -f /dev/null
              "

  # ========== 7) Oracle ==========
  oracle:
    container_name: oracle-oltp
    image: ${ORACLE_OLTP_IMAGE}
    hostname: oracle-oltp
    networks:
      bigdata-net:
        ipv4_address: ${ORACLE_OLTP_IP}
    ports:
      - "${LOCALHOST_ORACLE_CONNECT}:${CONTAINER_ORACLE_CONNECT}" # default port used for database connections
      - "${LOCALHOST_ORACLE_MANAGE_CONSOLE}:${CONTAINER_ORACLE_MANAGE_CONSOLE}" #  associated with the Oracle Enterprise Manager Console
    environment:
      - ORACLE_SID=${ORACLE_SID}
      - ORACLE_PDB=${ORACLE_PDB}
      - ORACLE_PWD=${ORACLE_PWD}
      - ORACLE_CHARACTERSET=${ORACLE_CHARACTERSET}

  # ========== 8) Airflow ==========
  airflow:
    container_name: airflow
    image: ${AIRFLOW_IMAGE}
    hostname: airflow
    networks:
      bigdata-net:
        ipv4_address: ${AIRFLOW_IP}
    # environment:
    #   - _AIRFLOW_WWW_USER_USERNAME=admin
    #   - _AIRFLOW_WWW_USER_PASSWORD=admin
    ports:
      - "${LOCALHOST_AIRFLOW_WEB_UI}:${CONTAINER_AIRFLOW_WEB_UI}"
    # volumes:
    #   - ./src/infra/airflow-configs/start_airflow.sh:${AIRFLOW_HOME}/start_airflow.sh
    command: >
      bash -c "/opt/airflow/start_airflow.sh;
               tail -f /dev/null"