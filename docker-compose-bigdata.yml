networks:
  bigdata-net:
    name: bigdata-net
    driver: bridge
    ipam:
      config:
        - subnet: ${HADOOP_SUBNET}  # subnet
          gateway: ${HADOOP_GATEWAY}    # GATEWAY

services:
  # ========== 1) Hadoop Master ==========
  # -------------------------------------------------
  # Hadoop Master: start ZK、JournalNode、NameNode、ZKFC、ResourceManager、DN/NM
  # -------------------------------------------------
  hadoop-master:
    container_name: hadoop-master               
    image: hadoop-master-image                         
    hostname: hadoop-master
    networks:
      bigdata-net:
        ipv4_address: ${HADOOP_MASTER_IP}
    environment: # Environment Variables
      - HADOOP_MASTER_HOST=hadoop-master
    ports:
      - "2222:${SSH_PORT}"    # SSH 
      - "${HDFS_WEB_UI}:${HDFS_WEB_UI}"
      - "${YARN_RM1_WEB_UI}:${YARN_RM1_WEB_UI}"
      - "8042:${YARN_LOG_WEB_UI}" 
      - "${HDFS}:${HDFS}" # HDFS for hive
      - "${YARN_JOB_HISTORY_SERVER}:${YARN_JOB_HISTORY_SERVER}"
    extra_hosts:
      - "hadoop-worker2:${HADOOP_WORKER2_IP}"
      - "hadoop-worker1:${HADOOP_WORKER1_IP}"
    stdin_open: true     # keep the container runing
    tty: true     # keep the container runing
    command: >
        bash -c "
          service ssh start;
          tail -f /dev/null
        "
    volumes:
      - ./infra/ssh-configs/configs:/root/.ssh/configs
      # hadoop cluster configs
      - ./infra/hadoop-configs/core-site.xml:${HADOOP_HOME}/etc/hadoop/core-site.xml
      - ./infra/hadoop-configs/hadoop-env.sh:${HADOOP_HOME}/etc/hadoop/hadoop-env.sh
      - ./infra/hadoop-configs/hdfs-site.xml:${HADOOP_HOME}/etc/hadoop/hdfs-site.xml
      - ./infra/hadoop-configs/yarn-site.xml:${HADOOP_HOME}/etc/hadoop/yarn-site.xml
      - ./infra/hadoop-configs/mapred-site.xml:${HADOOP_HOME}/etc/hadoop/mapred-site.xml
      - ./infra/hadoop-configs/yarn-env.sh:${HADOOP_HOME}/etc/hadoop/yarn-env.sh
      - ./infra/hadoop-configs/workers:${HADOOP_HOME}/etc/hadoop/workers
      - ./infra/zookeeper-configs/zoo.cfg:${ZOOKEEPER_HOME}/conf/zoo.cfg
      - ./infra/zookeeper-configs/dataDir.1/myid:${ZOOKEEPER_HOME}/data/myid
      # hdfs metadata
      - ./infra/hdfs-data/master:${HADOOP_HOME}/data
      # local mount - for big volume storage
      - /Users/smars/bigdata-cluster-volume/hadoop-master/data:/data
      
  # ========== 2) Hadoop Worker1 ==========
  # -------------------------------------------------
  # Hadoop Worker1: start ZK、JournalNode、ResourceManager、DN/NM
  # -------------------------------------------------
  hadoop-worker1:
    container_name: hadoop-worker1
    image: hadoop-worker1-image
    hostname: hadoop-worker1
    networks:
      bigdata-net:
        ipv4_address: ${HADOOP_WORKER1_IP}
    environment:
      - HADOOP_MASTER_HOST=hadoop-master
    ports:
      - "2223:${SSH_PORT}"    # SSH
      - "${YARN_RM2_WEB_UI}:${YARN_RM2_WEB_UI}"    # YARN_RM2_WEB_UI
      - "8043:${YARN_LOG_WEB_UI}"
    extra_hosts:
      - "hadoop-worker2:${HADOOP_WORKER2_IP}"
      - "hadoop-master:${HADOOP_MASTER_IP}"
    stdin_open: true     # keep the container runing
    tty: true     # keep the container runing
    command: >
      bash -c "service ssh start;
              tail -f /dev/null" 
    volumes:
      - ./infra/ssh-configs/configs:/root/.ssh/configs
      # hadoop cluster configs
      - ./infra/hadoop-configs/core-site.xml:${HADOOP_HOME}/etc/hadoop/core-site.xml
      - ./infra/hadoop-configs/hadoop-env.sh:${HADOOP_HOME}/etc/hadoop/hadoop-env.sh
      - ./infra/hadoop-configs/hdfs-site.xml:${HADOOP_HOME}/etc/hadoop/hdfs-site.xml
      - ./infra/hadoop-configs/mapred-site.xml:${HADOOP_HOME}/etc/hadoop/mapred-site.xml
      - ./infra/hadoop-configs/yarn-site.xml:${HADOOP_HOME}/etc/hadoop/yarn-site.xml
      - ./infra/hadoop-configs/workers:${HADOOP_HOME}/etc/hadoop/workers
      - ./infra/hadoop-configs/yarn-env.sh:${HADOOP_HOME}/etc/hadoop/yarn-env.sh
      - ./infra/zookeeper-configs/zoo.cfg:${ZOOKEEPER_HOME}/conf/zoo.cfg
      - ./infra/zookeeper-configs/dataDir.2/myid:${ZOOKEEPER_HOME}/data/myid
      # hdfs metadata
      - ./infra/hdfs-data/worker1:${HADOOP_HOME}/data
      # local mount - for big volume storage
      - /Users/smars/bigdata-cluster-volume/hadoop-worker1/data:/data

  # ========== 3) Hadoop Worker2 ==========
  # -------------------------------------------------
  # Hadoop Master: start ZK、JournalNode、NameNode、ZKFC、DN/NM
  # -------------------------------------------------
  hadoop-worker2:
    container_name: hadoop-worker2
    image: hadoop-worker2-image
    hostname: hadoop-worker2
    networks:
      bigdata-net:
        ipv4_address: ${HADOOP_WORKER2_IP}
    environment:
      - HADOOP_MASTER_HOST=hadoop-master
    ports:
      - "2224:${SSH_PORT}"    # SSH
      - "9871:${HDFS_WEB_UI}"
      - "8044:${YARN_LOG_WEB_UI}"
    extra_hosts:
      - "hadoop-master:${HADOOP_MASTER_IP}"
      - "hadoop-worker1:${HADOOP_WORKER1_IP}"
    stdin_open: true     # keep the container runing
    tty: true     # keep the container runing
    command: >
      bash -c "service ssh start;
              tail -f /dev/null" 
    volumes:
      - ./infra/ssh-configs/configs:/root/.ssh/configs
      # hadoop cluster configs
      - ./infra/hadoop-configs/core-site.xml:${HADOOP_HOME}/etc/hadoop/core-site.xml
      - ./infra/hadoop-configs/hadoop-env.sh:${HADOOP_HOME}/etc/hadoop/hadoop-env.sh
      - ./infra/hadoop-configs/hdfs-site.xml:${HADOOP_HOME}/etc/hadoop/hdfs-site.xml
      - ./infra/hadoop-configs/mapred-site.xml:${HADOOP_HOME}/etc/hadoop/mapred-site.xml
      - ./infra/hadoop-configs/workers:${HADOOP_HOME}/etc/hadoop/workers
      - ./infra/hadoop-configs/yarn-site.xml:${HADOOP_HOME}/etc/hadoop/yarn-site.xml
      - ./infra/hadoop-configs/yarn-env.sh:${HADOOP_HOME}/etc/hadoop/yarn-env.sh
      - ./infra/zookeeper-configs/zoo.cfg:${ZOOKEEPER_HOME}/conf/zoo.cfg
      - ./infra/zookeeper-configs/dataDir.3/myid:${ZOOKEEPER_HOME}/data/myid
      # hdfs metadata
      - ./infra/hdfs-data/worker2:${HADOOP_HOME}/data
      # local mount - for big volume storage
      - /Users/smars/bigdata-cluster-volume/hadoop-worker2/data:/data

  # ========== 4) MySQL (metastore for Hive) ==========
  mysql:
    container_name: mysql-hive-metastore
    image: mysql-hive-metastore
    hostname: mysql-hive-metastore
    networks:
      bigdata-net:
        ipv4_address: ${MYSQL_IP}
    environment:
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}  # for local testing only
    ports:
      - "${MYSQL}:${MYSQL}"
    volumes:
      - ./infra/mysql-data:/var/lib/mysql   # Local Persistence

  # ========== 5) Hive Service (metastore + hiveserver2) ==========
  hive:
    container_name: hive
    image: hive
    platform: linux/amd64
    hostname: hive
    networks:
      bigdata-net:
        ipv4_address: ${HIVE_IP}
    depends_on:
      - mysql
      - hadoop-master
    ports:
      - "${HIVE_SERVER2_PORT}:${HIVE_SERVER2_PORT}"  # HiveServer2
      - "${HIVE_SERVER2_2ND_PORT}:${HIVE_SERVER2_2ND_PORT}"  # HiveServer2 2nd
      - "${HIVE_METASTORE_PORT}:${HIVE_METASTORE_PORT}"    # Metastore Thrift
    volumes:
      - ./infra/hive-configs/hive-site.xml:${HIVE_HOME}/conf/hive-site.xml
      - ./infra/hive-configs/hiveservices.sh:${HIVE_HOME}/bin/hiveservices.sh
      - ./infra/hive-configs/mysql-connector-java-8.0.30.jar:${HIVE_HOME}/lib/mysql-connector-java-8.0.30.jar # JDBC
      - ./infra/hive-configs/hadoop-env.sh:${HIVE_HADOOP_HOME}/etc/hadoop/hadoop-env.sh
      - ./infra/hive-configs/core-site.xml:${HIVE_HADOOP_HOME}/etc/hadoop/core-site.xml
      - ./infra/hive-configs/hdfs-site.xml:${HIVE_HADOOP_HOME}/etc/hadoop/hdfs-site.xml
      - ./infra/hive-configs/core-site.xml:${HIVE_HOME}/conf/core-site.xml
      - ./infra/hive-configs/hdfs-site.xml:${HIVE_HOME}/conf/hdfs-site.xml
    environment:
      # connect metastore
      - JAVA_HOME=${HIVE_JDK} # different with hadoop cluster cuz hive 3.1.3 need jdk8 and amd64 for the container
      - PATH=$JAVA_HOME/bin:$PATH
      - HIVE_METASTORE_DB_HOST=${METASTORE_HOST}
      - HIVE_METASTORE_DB_USER=${METASTORE_USERNAME}
      - HIVE_METASTORE_DB_PASS=${METASTORE_PASSWORD}
    # command: >
    #   bash -c "/opt/hive/bin/hiveservices.sh start; 
    #           tail -f /dev/null" 

  # ========== 6) Spark on YARN ==========
  spark:
    container_name: spark
    image: spark
    hostname: spark
    networks:
      bigdata-net:
        ipv4_address: ${SPARK_IP}
    depends_on: # launch the hadoop cluster first
      - hadoop-master
      - hadoop-worker1
      - hadoop-worker2
      - hive
    ports:
      - "2226:${SSH_PORT}"    # SSH 
      - "${SPARK_WEB_UI}:${SPARK_WEB_UI}"  # Spark UI
      - "${SPARK_MASTER}:${SPARK_MASTER}"  # Spark Master
    volumes:
      - ./infra/spark-configs/spark-defaults.conf:${SPARK_HOME}/conf/spark-defaults.conf
      - ./infra/spark-configs/spark-env.sh:${SPARK_HOME}/conf/spark-env.sh
      - ./infra/spark-configs/log4j2.properties:${SPARK_HOME}/conf/log4j2.properties
      - ./infra/hadoop-configs/core-site.xml:${SPARK_HOME}/conf/core-site.xml
      - ./infra/hadoop-configs/yarn-site.xml:${SPARK_HOME}/conf/yarn-site.xml
      - ./infra/hadoop-configs/hdfs-site.xml:${SPARK_HOME}/conf/hdfs-site.xml
    environment:
      - SPARK_MASTER=hadoop-master
      - SPARK_WORKLOAD=yarn
    command: >
      bash -c "
              service ssh start;
              tail -f /dev/null
              "

  # ========== 7) Oracle ==========
  oracle:
    container_name: oracle-oltp
    image: oracle-oltp
    hostname: oracle-oltp
    networks:
      bigdata-net:
        ipv4_address: ${ORACLE_OLTP_IP}
    ports:
      - "${ORACLE_CONNECT}:${ORACLE_CONNECT}" # default port used for database connections
      - "${ORACLE_MANAGE_CONSOLE}:${ORACLE_MANAGE_CONSOLE}" #  associated with the Oracle Enterprise Manager Console
    environment:
      - ORACLE_SID=${ORACLE_SID}
      - ORACLE_PDB=${ORACLE_PDB}
      - ORACLE_PWD=${ORACLE_PWD}
      - ORACLE_CHARACTERSET=${ORACLE_CHARACTERSET}

  # ========== 8) Airflow ==========
  airflow:
    container_name: airflow
    image: airflow
    hostname: airflow
    networks:
      bigdata-net:
        ipv4_address: ${AIRFLOW_IP}
    # environment:
    #   - _AIRFLOW_WWW_USER_USERNAME=admin
    #   - _AIRFLOW_WWW_USER_PASSWORD=admin
    ports:
      - "${AIRFLOW_WEB_UI}:${AIRFLOW_WEB_UI}"
    # volumes:
    #   - ./infra/airflow-configs/start_airflow.sh:${AIRFLOW_HOME}/start_airflow.sh
    command: >
      bash -c "/opt/airflow/start_airflow.sh;
               tail -f /dev/null"