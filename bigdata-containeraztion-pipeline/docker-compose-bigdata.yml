networks:
  bigdata-net:
    name: bigdata-net
    driver: bridge
    ipam:
      config:
        - subnet: ${HADOOP_SUBNET}  # subnet
          gateway: ${HADOOP_GATEWAY}    # GATEWAY

services:
  # ========== 1) Hadoop Master ==========
  hadoop-master:
    container_name: hadoop-master               
    image: hadoop-master-image                         
    hostname: hadoop-master
    networks:
      bigdata-net:
        ipv4_address: ${HADOOP_MASTER_IP}
    environment: # Environment Variables
      - HADOOP_MASTER_HOST=hadoop-master
    ports:
      - "2222:${SSH_PORT}"    # SSH 
      - "${HDFS_WEB_UI}:${HDFS_WEB_UI}"
      - "${HDFS}:${HDFS}" # HDFS for hive
    extra_hosts:
      - "hadoop-worker2:${HADOOP_WORKER2_IP}"
      - "hadoop-worker1:${HADOOP_WORKER1_IP}"
    stdin_open: true     # keep the container runing
    tty: true     # keep the container runing
    command: >
      bash -c "service ssh start;
              bash -l -c "/usr/local/opt/module/hadoop/sbin/start-dfs.sh";
              tail -f /dev/null" 
    volumes:
    #   - ./hadoop-config:/usr/local/hadoop/etc/hadoop
      - ./ssh-config/config:/root/.ssh/config
    
  # ========== 2) Hadoop Worker1 ==========
  hadoop-worker1:
    container_name: hadoop-worker1
    image: hadoop-worker1-image
    hostname: hadoop-worker1
    networks:
      bigdata-net:
        ipv4_address: ${HADOOP_WORKER1_IP}
    environment:
      - HADOOP_MASTER_HOST=hadoop-master
    ports:
      - "2223:${SSH_PORT}"    # SSH
      - "${YARN_WEB_UI}:${YARN_WEB_UI}"  # YARN ResourceManager web UI 
    extra_hosts:
      - "hadoop-worker2:${HADOOP_WORKER2_IP}"
      - "hadoop-master:${HADOOP_MASTER_IP}"
    stdin_open: true     # keep the container runing
    tty: true     # keep the container runing
    command: >
      bash -c "service ssh start;
              bash -l -c "/usr/local/opt/module/hadoop/sbin/start-yarn.sh";
              tail -f /dev/null" 
    volumes:
    #   - ./hadoop-config:/usr/local/hadoop/etc/hadoop
      - ./ssh-config/config:/root/.ssh/config
  # ========== 3) Hadoop Worker2 ==========
  hadoop-worker2:
    container_name: hadoop-worker2
    image: hadoop-worker2-image
    hostname: hadoop-worker2
    networks:
      bigdata-net:
        ipv4_address: ${HADOOP_WORKER2_IP}
    environment:
      - HADOOP_MASTER_HOST=hadoop-master
    ports:
      - "2224:${SSH_PORT}"    # SSH
    extra_hosts:
      - "hadoop-master:${HADOOP_MASTER_IP}"
      - "hadoop-worker1:${HADOOP_WORKER1_IP}"
    stdin_open: true     # keep the container runing
    tty: true     # keep the container runing
    command: >
      bash -c "service ssh start; 
              tail -f /dev/null" 
    volumes:
    #   - ./hadoop-config:/usr/local/hadoop/etc/hadoop
      - ./ssh-config/config:/root/.ssh/config

  # ========== 4) MySQL (metastore for Hive) ==========
  mysql:
    container_name: mysql-hive-metastore
    image: ubuntu/mysql
    hostname: mysql-hive-metastore
    networks:
      bigdata-net:
        ipv4_address: ${MYSQL_IP}
    environment:
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}  # for local testing only
    ports:
      - "${MYSQL}:${MYSQL}"
    volumes:
      - ./mysql-data:/var/lib/mysql   # Persistence to local

  # ========== 5) Hive Service (metastore + hiveserver2) ==========
  hive:
    build: ./hive-config # custom dockerfile
    container_name: hive
    image: apache/hive
    platform: linux/amd64
    hostname: hive
    networks:
      bigdata-net:
        ipv4_address: ${HIVE_IP}
    depends_on:
      - mysql
      - hadoop-master
    ports:
      - "${HIVE_SERVER2_PORT}:${HIVE_SERVER2_PORT}"  # HiveServer2
      - "${HIVE_SERVER2_2ND_PORT}:${HIVE_SERVER2_2ND_PORT}"  # HiveServer2 2nd
      - "${HIVE_METASTORE_PORT}:${HIVE_METASTORE_PORT}"    # Metastore Thrift
    volumes:
      - ./hive-config/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./hive-config/hiveservices.sh:/opt/hive/bin/hiveservices.sh
      - ./hive-config/mysql-connector-java-8.0.30.jar:/opt/hive/lib/mysql-connector-java-8.0.30.jar # JDBC
      - ./hive-config/hadoop-env.sh:/opt/hadoop/etc/hadoop/hadoop-env.sh
      - ./hive-config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
    environment:
      # connect metastore
      - JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 # different with hadoop cluster cuz hive 3.1.3 need jdk8 and amd64 for the container
      - PATH=$JAVA_HOME/bin:$PATH
      - HIVE_METASTORE_DB_HOST=${METASTORE_HOST}
      - HIVE_METASTORE_DB_USER=${METASTORE_USERNAME}
      - HIVE_METASTORE_DB_PASS=${METASTORE_PASSWORD}
    # command: >
    #   bash -c "hiveservices.sh start; 
    #           tail -f /dev/null" 

  # ========== 6) Spark on YARN ==========
  # spark:
  #   container_name: spark
  #   image: your_spark_image
  #   hostname: spark
  #   networks:
  #     - bigdata-net
  #   depends_on:
  #     - hadoop-master
  #   volumes:
  #     - ./core-site.xml:/opt/hadoop/conf/core-site.xml
  #     - ./yarn-site.xml:/opt/hadoop/conf/yarn-site.xml
  #   # environment / entrypoint
