# Error Handling Log: NodeManager Disk Space Issue Preventing YARN Registration

---

## Overview

This document summarizes an issue encountered in our Hadoop cluster where the NodeManager failed to register with YARN due to insufficient disk space. This log details the observed error, the root cause analysis, and the solution implemented by using Docker volume binding for local storage.

---

## Encountered Issue

When attempting to start the NodeManager on our cluster nodes, the YARN ResourceManager reported zero available nodes. The NodeManager logs contained messages similar to:

```
Directory /tmp/hadoop-root/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
Directory /usr/local/opt/module/hadoop/logs/userlogs error, used space above threshold of 90.0%, removing from list of valid directories
Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /tmp/hadoop-root/nm-local-dir : used space above threshold of 90.0% ]
```

The YARN Web UI showed:
```
Total Nodes: 0
```
indicating that no NodeManager had been registered, which caused Spark jobs to remain in the ACCEPTED state and eventually fail with errors such as exit code 13.

---

## Root Cause Analysis

After thorough investigation, the root cause was identified as follows:

- **Disk Space Overutilization:**  
  The container's root file system (using the overlay driver) was nearly full. The `df` command revealed that the disk usage was around 92%, which exceeds the NodeManager's threshold (default is 90%) for healthy disk usage.

- **NodeManager Disk Health Check:**  
  YARN’s NodeManager performs a disk health check on its local directories (for both local-dirs and log-dirs). When disk usage exceeds the configured threshold, those directories are deemed "unusable," and NodeManager fails to register with the ResourceManager due to a lack of valid resources.

- **Environment Limitation:**  
  In our Docker deployment, the container’s root filesystem is limited, and the NodeManager was using the container's default storage, which quickly reached high utilization levels.

---

## Solution

The chosen solution was to **mount a Docker volume** from the host with sufficient capacity into the container, so that the NodeManager uses the host's storage for its local directories rather than the container’s limited overlay filesystem.

### Implementation Details

1. **Docker Volume/Bind Mount:**  
   We configured the Docker Compose file to mount a host directory with ample storage. Example:

    ```yml
        volumes:
            ....
            # local mount - for big volume storage
            - /Users/smars/bigdata-cluster-volume/hadoop-worker2/data:/data
        ....
    ```
2. **Update YARN Configuration:**  
   The `yarn-site.xml` was updated to point the `yarn.nodemanager.local-dirs` and `yarn.nodemanager.log-dirs` to the mounted volume paths:
   ```xml
   <property>
     <name>yarn.nodemanager.local-dirs</name>
     <value>/data/nm-local-dir</value>
   </property>
   <property>
     <name>yarn.nodemanager.log-dirs</name>
     <value>/data/nm-logs</value>
   </property>
   ```

3. **Restart NodeManager:**  
   After updating the Docker Compose configuration and YARN settings, all NodeManager processes were restarted. This allowed them to register successfully with the ResourceManager, as the local directories now had sufficient space.

---

## Conclusion

By mounting a high-capacity Docker volume and updating the YARN configuration to use these mounted directories for NodeManager storage, we resolved the disk space overutilization issue. The NodeManagers were able to register correctly with YARN, enabling Spark applications to receive proper resource allocations and run successfully.

This incident underscores the importance of ensuring that containers running resource-intensive components like NodeManager have access to adequate storage. Future deployments will include pre-checks for disk utilization and improved Docker volume management to prevent similar issues.

---

*End of Error Handling Log*