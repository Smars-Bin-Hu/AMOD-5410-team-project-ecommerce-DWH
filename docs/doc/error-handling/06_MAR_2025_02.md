# Troubleshooting: Docker Container Running Out of Memory

## Description of the Issue
While running a Spark job on a YARN cluster inside Docker containers, the job was killed due to insufficient memory allocation. The error logs included warnings such as:

```
WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
Killed
```

Additionally, checking the available system memory with `free -h` showed that memory usage was close to the limit, with almost no available RAM left and swap fully utilized.

## Root Cause Analysis

1. **Docker Memory Allocation Issue**: By default, Docker Desktop (on macOS) allocates a limited amount of memory to all running containers. Even if the host machine has more available RAM (e.g., 32GB), Docker might restrict its usage.
   
2. **Insufficient Memory for Spark Executors**: The Spark job requested memory via:
   ```
   --executor-memory 512m
   --executor-cores 1
   --num-executors 1
   ```
   However, given the overall cluster load and Docker's memory restrictions, the executors were unable to get allocated memory from YARN.

3. **High Memory Usage from Other Running Containers**: Multiple Docker containers were running on the system, consuming available resources and causing additional memory pressure.

4. **Swap Exhaustion**: The swap was already fully utilized (`Swap: 1.0Gi Used 1.0Gi`), which means there was no fallback for processes requiring additional memory.

## Solution: Adjusting Docker Memory Allocation
### **1. Increase Global Docker Memory Allocation (Docker Desktop for macOS)**
Since Docker Desktop manages all containers within a virtual machine, increasing the allocated memory helps containers run larger workloads.

- **Steps:**
  1. Open **Docker Desktop**
  2. Go to **Settings** → **Resources** → **Memory**
  3. Increase memory allocation (e.g., from 8GB to 16GB or more, depending on the available host memory)
  4. Click **Apply & Restart**

### **2. Set Memory Limits for Individual Containers**
If only a specific container requires more memory, explicitly allocate memory to it.

- **For `docker run`**:
  ```bash
  docker run -it --name my-spark-container -m 8g --memory-swap=8g my-spark-image
  ```

- **For `docker-compose.yml`**:
  ```yaml
  services:
    spark-container:
      image: my-spark-image
      deploy:
        resources:
          limits:
            memory: 8g
  ```


## Conclusion
By increasing Docker's memory allocation and ensuring Spark executors request reasonable memory sizes, the issue of memory starvation and job failures due to resource constraints was resolved. Monitoring Docker resource usage (`docker stats`) and adjusting memory limits accordingly can help prevent similar issues in the future.

