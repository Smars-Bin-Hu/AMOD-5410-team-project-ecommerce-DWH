# **Troubleshooting Hive Unable to Read Parquet Files Written by Spark SQL**

## Description
When using **Spark SQL** to write data to a Hive-managed **Parquet table** via `INSERT OVERWRITE TABLE`, Hive fails to read the resulting Parquet files and throws the following error:

```text
java.io.IOException: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file hdfs://ns-ha/user/hive/warehouse/dwd/dwd_campaign_product_subcategory_fpd/part-00000-xxx.snappy.parquet
```

However, when reading the same Parquet files with Spark (`spark.read.parquet(...)`), the data is successfully loaded, indicating that the files are not corrupted.

---

## Root Cause

### **1. Spark Writes Parquet Using a Different Decimal Encoding**
- **Spark 3.x** introduced a new **Decimal encoding format** that differs from the **Legacy format** used by Hive.
- When `INSERT OVERWRITE TABLE` is executed in Spark SQL, it writes **Parquet Decimal values in a format that Hive does not recognize**, leading to `ParquetDecodingException`.
- When writing Parquet manually using `df.write.parquet(...)`, Spark allows explicit control over the encoding format, which is why manually written files may work with Hive.

### **2. Legacy Format Compatibility Issue**
- Hive 3.1.3 expects **Parquet Decimal columns** to be stored in the **Legacy format**.
- By default, **Spark 3.x writes Parquet in a newer format**, which is **not backward compatible** with Hive.

---

## Solution
To ensure that **Spark SQL writes Parquet files in a format compatible with Hive**, explicitly enable the **Legacy Decimal Format** before executing `INSERT OVERWRITE TABLE`.

### **Fix: Set `spark.sql.parquet.writeLegacyFormat` to `true`**
Modify your **Spark session configuration** as follows:

```python
from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("UT - ods_to_dwd")
    .enableHiveSupport()  # Ensure Hive compatibility
    .config("spark.sql.parquet.writeLegacyFormat", "true")  # Enable legacy Decimal encoding
    .getOrCreate()
)

sql = """
INSERT OVERWRITE TABLE dwd.dwd_campaign_product_subcategory_fpd
SELECT
    campaign_product_subcategory_id,
    campaign_id,
    subcategory_id,
    discount
FROM
    ods.ods_campaign_product_subcategory_fpd;
"""

spark.sql(sql)
spark.stop()
```

### Additional Configurations (if needed)
If your table contains **TIMESTAMP or DATE fields**, you may also need:
```python
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.int96RebaseModeInWrite", "CORRECTED")
```

---

## Summary
| Issue | Cause | Solution |
|-------|-------|----------|
| **Hive fails to read Spark-generated Parquet files** | Spark 3.x writes **Decimal fields** using a new encoding that is incompatible with Hive 3.1.3 | Set `spark.sql.parquet.writeLegacyFormat=true` **before executing `INSERT OVERWRITE TABLE`** |
| **ParquetDecodingException when reading in Hive** | Hive expects **Legacy Decimal Format**, but Spark writes in a new format | Use `spark.write.parquet(...)` with `writeLegacyFormat=true` |
| **Timestamps or Dates not correctly interpreted** | Hive and Spark have different handling for TIMESTAMP fields | Set `spark.sql.legacy.parquet.datetimeRebaseModeInWrite=CORRECTED` |

By applying these solutions, Spark-generated Parquet files will be **fully compatible with Hive**, eliminating `ParquetDecodingException`.

