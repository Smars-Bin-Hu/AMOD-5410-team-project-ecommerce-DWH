# Troubleshooting: Spark on Yarn submit encounter Container exited with a non-zero exit code 13. Error file: prelaunch.err.

## Problem

run below spark command,

```bash
spark-submit --master yarn \
    --deploy-mode cluster \
    --driver-memory 512m \
    --executor-memory 1g \
    --executor-cores 1 \
    --num-executors 1 \
    --conf "spark.yarn.executor.memoryOverhead=512" \
    --conf "spark.yarn.driver.memoryOverhead=256" \
    --conf "spark.driver.extraClassPath=/opt/spark/jars/ojdbc8.jar" \
    /opt/miniconda3/envs/pyspark_env/jobs_sync/Users/smars/Developer/big-data-engineering-project1/data_pipeline/unit_test/spark_connect_oracle.py
```

encounter error:

```bash
....
2025-03-05 02:18:57 INFO Client: Application report for application_1741156980683_0004 (state: ACCEPTED)
2025-03-05 02:18:58 INFO Client: Application report for application_1741156980683_0004 (state: ACCEPTED)
2025-03-05 02:18:59 INFO Client: Application report for application_1741156980683_0004 (state: ACCEPTED)
2025-03-05 02:19:00 INFO Client: Application report for application_1741156980683_0004 (state: FAILED)
2025-03-05 02:19:00 INFO Client: 
         client token: N/A
         diagnostics: Application application_1741156980683_0004 failed 2 times due to AM Container for appattempt_1741156980683_0004_000002 exited with  exitCode: 13
Failing this attempt.Diagnostics: [2025-03-05 02:18:59.804]Exception from container-launch.
Container id: container_1741156980683_0004_02_000001
Exit code: 13

[2025-03-05 02:18:59.806]Container exited with a non-zero exit code 13. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :


[2025-03-05 02:18:59.807]Container exited with a non-zero exit code 13. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :


For more detailed output, check the application tracking page: http://hadoop-worker1:8089/cluster/app/application_1741156980683_0004 Then click on links to logs of each attempt.
. Failing the application.
         ApplicationMaster host: N/A
         ApplicationMaster RPC port: -1
         queue: default
         start time: 1741159072849
         final status: FAILED
         tracking URL: http://hadoop-worker1:8089/cluster/app/application_1741156980683_0004
         user: root
2025-03-05 02:19:00 ERROR Client: Application diagnostics message: Application application_1741156980683_0004 failed 2 times due to AM Container for appattempt_1741156980683_0004_000002 exited with  exitCode: 13
Failing this attempt.Diagnostics: [2025-03-05 02:18:59.804]Exception from container-launch.
Container id: container_1741156980683_0004_02_000001
Exit code: 13

[2025-03-05 02:18:59.806]Container exited with a non-zero exit code 13. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :


[2025-03-05 02:18:59.807]Container exited with a non-zero exit code 13. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :


For more detailed output, check the application tracking page: http://hadoop-worker1:8089/cluster/app/application_1741156980683_0004 Then click on links to logs of each attempt.
. Failing the application.
Exception in thread "main" org.apache.spark.SparkException: Application application_1741156980683_0004 finished with failed status
        at org.apache.spark.deploy.yarn.Client.run(Client.scala:1342)
        at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1764)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2025-03-05 02:19:00 INFO ShutdownHookManager: Shutdown hook called
2025-03-05 02:19:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-9b5aa15d-b9a1-4cf2-b896-93b8748b97b7
2025-03-05 02:19:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd219153-4610-4d41-8f51-a3493f5db66a

```

Yarn Web UI Diagnostics

```bash
Diagnostics:
Application application_1741156980683_0003 failed 2 times due to AM Container for appattempt_1741156980683_0003_000002 exited with exitCode: 13
Failing this attempt.Diagnostics: [2025-03-05 02:03:33.529]Exception from container-launch.
Container id: container_1741156980683_0003_02_000001
Exit code: 13
[2025-03-05 02:03:33.544]Container exited with a non-zero exit code 13. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :
[2025-03-05 02:03:33.544]Container exited with a non-zero exit code 13. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :
For more detailed output, check the application tracking page: http://hadoop-worker1:8089/cluster/app/application_1741156980683_0003 Then click on links to logs of each attempt.
. Failing the application.
```

Root Cause: based on Yarn Web UI application attempt logs

```bash
...
Caused by: java.io.IOException: Cannot run program "/opt/miniconda3/envs/pyspark_env/bin/python": error=2, No such file or directory
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1128) ~[?:?]
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1071) ~[?:?]
...
```

My Spark Python Env: `/opt/miniconda3/envs/pyspark_env/bin/python`
My NodeManager Python Env: `/usr/local/python3.8` instead of `/opt/miniconda3/envs/pyspark_env/bin/python`

try to solve by using below command with conf for `spark.executorEnv.PYSPARK_PYTHON` and `spark.yarn.appMasterEnv.PYSPARK_PYTHON`, but failed. because the global env variable on the Spark docker container cannot be overwrite by below command.

```bash
spark-submit --master yarn \
    --deploy-mode cluster \
    --driver-memory 512m \
    --executor-memory 1g \
    --executor-cores 1 \
    --num-executors 1 \
    --conf "spark.yarn.executor.memoryOverhead=512" \
    --conf "spark.yarn.driver.memoryOverhead=256" \
    --conf "spark.executorEnv.PYSPARK_PYTHON=/usr/bin/python3.8" \
    --conf "spark.yarn.appMasterEnv.PYSPARK_PYTHON=/usr/bin/python3.8" \
    --conf "spark.driver.extraClassPath=/opt/spark/jars/ojdbc8.jar" \
    /opt/miniconda3/envs/pyspark_env/jobs_sync/Users/smars/Developer/big-data-engineering-project1/data_pipeline/unit_test/spark_connect_oracle.py

```

## Final Solution

for all nodemanagers `hadoop-worker1` `hadoop-master` `hadoop-worker2`,
Install the miniconda and create pyspark_env

```bash
/opt/miniconda3/bin/conda create -y --name pyspark_env python=3.8
```

activate the virtual env

```bash
source /opt/miniconda3/bin/activate pyspark_env
```

config the env virable

```bash
export PYSPARK_PYTHON=/opt/miniconda3/envs/pyspark_env/bin/python
export PYSPARK_DRIVER_PYTHON=/opt/miniconda3/envs/pyspark_env/bin/python
```
