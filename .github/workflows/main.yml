name: Data Warehouse Cluster Container Deployment and Services Launching

on:
  workflow_dispatch:

jobs:
  hadoop-cluster-start:
    runs-on: self-hosted
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Login to GitHub Container Registry
        run: echo "${{ secrets.GHCR_TOKEN }}" | docker login ghcr.io -u smars-bin-hu --password-stdin

      - name: Pull Docker Images from Docker Hub
        run: |
          docker pull ghcr.io/smars-bin-hu/proj1-dwh-cluster:hadoop-master-smars-1.1.2
          docker pull ghcr.io/smars-bin-hu/proj1-dwh-cluster:hadoop-worker1-smars-1.1.2
          docker pull ghcr.io/smars-bin-hu/proj1-dwh-cluster:hadoop-worker2-smars-1.1.2
          docker pull ghcr.io/smars-bin-hu/proj1-dwh-cluster:mysql-hive-metastore-smars-1.1.2
          docker pull ghcr.io/smars-bin-hu/proj1-dwh-cluster:hive-smars-1.1.2
          docker pull ghcr.io/smars-bin-hu/proj1-dwh-cluster:spark-smars-1.1.1
          docker pull ghcr.io/smars-bin-hu/proj1-dwh-cluster:oracle-oltp-smars-1.1.1
          docker pull ghcr.io/smars-bin-hu/proj1-dwh-cluster:airflow-smars-1.1.1

      - name: Start Big Data Cluster with Docker Compose
        run: |
          docker compose -f docker-compose-bigdata.yml up -d

      - name: Show Running Containers (debug)
        run: |
          docker ps -a

      - name: Start Hadoop HA Cluster
        run: |
          bash start-hadoop-cluster.sh

      - name: Check JPS on hadoop-master
        run: |
          processes=$(docker exec hadoop-master jps)
          echo "$processes" | grep -q "ResourceManager" || exit 1
          echo "$processes" | grep -q "NameNode" || exit 1
          echo "$processes" | grep -q "DataNode" || exit 1
          echo "$processes" | grep -q "JournalNode" || exit 1
          echo "$processes" | grep -q "QuorumPeerMain" || exit 1
          echo "$processes" | grep -q "NodeManager" || exit 1
          echo "$processes" | grep -q "DFSZKFailoverController" || exit 1
        continue-on-error: false

      - name: Check JPS on hadoop-worker1
        run: |
          processes=$(docker exec hadoop-worker1 jps)
          echo "$processes" | grep -q "ResourceManager" || exit 1
          echo "$processes" | grep -q "DataNode" || exit 1
          echo "$processes" | grep -q "JournalNode" || exit 1
          echo "$processes" | grep -q "QuorumPeerMain" || exit 1
          echo "$processes" | grep -q "NodeManager" || exit 1
        continue-on-error: false

      - name: Check JPS on hadoop-worker2
        run: |
          processes=$(docker exec hadoop-worker2 jps)
          echo "$processes" | grep -q "NameNode" || exit 1
          echo "$processes" | grep -q "DFSZKFailoverController" || exit 1
          echo "$processes" | grep -q "DataNode" || exit 1
          echo "$processes" | grep -q "JournalNode" || exit 1
          echo "$processes" | grep -q "QuorumPeerMain" || exit 1
          echo "$processes" | grep -q "NodeManager" || exit 1
        continue-on-error: false

      - name: Check HDFS HA status
        run: |
          echo "HDFS nn1:" && docker exec -i hadoop-master bash -c "/usr/local/opt/module/hadoop/bin/hdfs haadmin -getServiceState nn1"
          echo "HDFS nn2:" && docker exec -i hadoop-master bash -c "/usr/local/opt/module/hadoop/bin/hdfs haadmin -getServiceState nn2"

      - name: Check YARN HA status
        run: |
          echo "YARN rm1:" && docker exec -i hadoop-master bash -c "/usr/local/opt/module/hadoop/bin/yarn rmadmin -getServiceState rm1"
          echo "YARN rm2:" && docker exec -i hadoop-master bash -c "/usr/local/opt/module/hadoop/bin/yarn rmadmin -getServiceState rm2"
  
  mysql-metadata-restore:
    runs-on: self-hosted
    needs: hadoop-cluster-start
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: restore MySQL Metastore
        run: |
          bash mysql-metadata-restore.sh

      - name: check the metastore database
        run: |
          metastore_dir=$(docker exec -i mysql-hive-metastore bash -c "ls -la /var/lib/mysql | grep -q metastore && echo found || echo not_found")
          if [ "$metastore_dir" = "not_found" ]; then
            echo "metastore database directory is not found"
            exit 1
          fi
          echo "metastore database directory exists"
        continue-on-error: false

  data-clients-start:
    runs-on: self-hosted
    needs: mysql-metadata-restore
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Start Data Clients - Hive Metastore, Hiveserver2, Spark ThriftServer
        run: |
          bash start-data-clients.sh

      - name: Check Hive container ports
        id: check_hive_ports
        run: |
          hive_ports=$(docker exec -i hive bash -c "/bin/netstat -nltp | grep -E '9083|10000|10002'")
          if [ -z "$hive_ports" ]; then
            echo "Hive ports are not open"
            exit 1
          fi
          echo "Hive ports are open"
        continue-on-error: false

      - name: Check Spark container ports
        id: check_spark_ports
        run: |
          spark_ports=$(docker exec -i spark bash -c "/bin/netstat -nltp | grep -E '4040|10000'")
          if [ -z "$spark_ports" ]; then
            echo "Spark ports are not open"
            exit 1
          fi
          echo "Spark ports are open"
        continue-on-error: false

  data-clients-stop:
    runs-on: self-hosted
    needs: data-clients-start
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Stop Data Clients - Hive Metastore, Hiveserver2, Spark ThriftServer
        run: |
          bash stop-data-clients.sh
      
      - name: Check Hive container ports
        id: check_hive_ports
        run: |
          hive_ports=$(docker exec -i hive bash -c "/bin/netstat -nltp | grep -E '9083|10000|10002'" || true)
          if [ ! -z "$hive_ports" ]; then
            echo "Hive ports 9083, 10000, or 10002 are still running"
            exit 1
          fi
          echo "Hive ports are closed"
        continue-on-error: false

      - name: Check Spark container ports
        id: check_spark_ports
        run: |
          spark_ports=$(docker exec -i spark bash -c "/bin/netstat -nltp | grep -E '4040|10000'"  || true)
          if [ ! -z "$spark_ports" ]; then
            echo "Spark ports 4040, 10000 are still running"
            exit 1
          fi
          echo "Spark ports are closed"
        continue-on-error: false

  hadoop-cluster-stop:
    runs-on: self-hosted
    needs: data-clients-stop
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Stop Hadoop HA Cluster
        run: |
          bash stop-hadoop-cluster.sh

      - name: Check JPS on hadoop-master
        run: |
          processes=$(docker exec hadoop-master jps || echo "")
          running_services=""
          
          for service in "ResourceManager" "NameNode" "DataNode" "JournalNode" "QuorumPeerMain" "NodeManager" "DFSZKFailoverController"; do
            if echo "$processes" | grep -q "$service"; then
              running_services="$running_services $service"
            fi
          done
          
          if [ ! -z "$running_services" ]; then
            echo "::warning::Following services are still running on hadoop-master:$running_services"
            echo "HADOOP_STOP_FAILED=true" >> $GITHUB_ENV
          else
            echo "All services on hadoop-master are stopped successfully"
          fi
        continue-on-error: false

      - name: Check JPS on hadoop-worker1
        run: |
          processes=$(docker exec hadoop-worker1 jps || echo "")
          running_services=""
          
          for service in "ResourceManager" "DataNode" "JournalNode" "QuorumPeerMain" "NodeManager"; do
            if echo "$processes" | grep -q "$service"; then
              running_services="$running_services $service"
            fi  
          done
          
          if [ ! -z "$running_services" ]; then
            echo "::warning::Following services are still running on hadoop-worker1:$running_services"
            echo "HADOOP_STOP_FAILED=true" >> $GITHUB_ENV
          else  
            echo "All services on hadoop-worker1 are stopped successfully"
          fi
        continue-on-error: false

      - name: Check JPS on hadoop-worker2
        run: |
          processes=$(docker exec hadoop-worker2 jps || echo "")
          running_services=""
          
          for service in "NameNode" "DFSZKFailoverController" "DataNode" "JournalNode" "QuorumPeerMain" "NodeManager"; do
            if echo "$processes" | grep -q "$service"; then
              running_services="$running_services $service"
            fi
          done
          
          if [ ! -z "$running_services" ]; then
            echo "::warning::Following services are still running on hadoop-worker2:$running_services"
            echo "HADOOP_STOP_FAILED=true" >> $GITHUB_ENV
          else
            echo "All services on hadoop-worker2 are stopped successfully"
          fi  
        continue-on-error: false
  
      - name: Check if hadoop cluster stopped successfully
        run: |
          if [ "${HADOOP_STOP_FAILED:-false}" == "true" ]; then
            echo "::error::Hadoop cluster stop operation failed, some services are still running"
            exit 1
          else
            echo "Hadoop cluster stopped successfully"
          fi
        continue-on-error: false
        
  # docker-compose-bigdata-stop:
  #   runs-on: self-hosted
  #   needs: [hadoop-cluster-start, mysql-metadata-restore, data-clients-start, data-clients-stop,  hadoop-cluster-stop]
  #   if: always()
  #   steps:
  #     - name: Checkout Repository
  #       uses: actions/checkout@v4
        
  #     - name: Stop Big Data Cluster with Docker Compose
  #       run: |
  #         docker compose -p ecomdwh-batchdataprocessingplatform -f docker-compose-bigdata.yml down -v

  #     - name: Show Running Containers (debug)
  #       run: |
  #         docker ps -a
